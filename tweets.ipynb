{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.6.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nltk) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspellchecker\n",
    "!pip3 install nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, path_to_data):\n",
    "        files = []\n",
    "        tweets = pd.DataFrame()\n",
    "        for filenames in os.listdir(path_to_data):\n",
    "            if re.findall('.+\\.csv', filenames):\n",
    "                files.append(filenames)\n",
    "        for file in files:\n",
    "            with open(path_to_data + '/' + file, 'r') as f:\n",
    "                data = f.read()\n",
    "            data = re.sub(r',(?=[^ ])', '--', data)\n",
    "            df = pd.DataFrame(data.split('--'), columns=['tweets'])\n",
    "            df['category'] = file\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            tweets = tweets.append(df)\n",
    "        tweets['category'].replace({\n",
    "            'processedNegative.csv' : -1, \n",
    "            'processedNeutral.csv': 0, \n",
    "            'processedPositive.csv' : 1}, inplace=True)\n",
    "        tweets.index = np.arange(len(tweets))\n",
    "        self.tweets = tweets\n",
    "        self.spell = SpellChecker()\n",
    "\n",
    "\n",
    "    def tokenization(self):\n",
    "        array = []\n",
    "        for sent in self.tweets.tweets:\n",
    "            array.append(word_tokenize(sent))\n",
    "        self.tweets[\"just_tokenization\"] = array\n",
    "\n",
    "    def PorterStemmer(self, misspellings=False):\n",
    "        array = []\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        for sent in self.tweets.tweets:\n",
    "            array.append(word_tokenize(sent))\n",
    "            for i, w in enumerate(array[-1]):\n",
    "                if (misspellings == True):\n",
    "                    array[-1][i] = self.spell.correction(w)\n",
    "                array[-1][i] = porter_stemmer.stem(w)\n",
    "    \n",
    "        if (misspellings):\n",
    "            self.tweets[\"PorterStemmer+misspellings\"] = array\n",
    "        else:\n",
    "            self.tweets[\"PorterStemmer\"] = array\n",
    "\n",
    "    def WordNetLemmatizer(self, misspellings=False):\n",
    "        nltk.download('wordnet')\n",
    "        array = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for sent in self.tweets.tweets:\n",
    "            array.append(word_tokenize(sent))\n",
    "            for i, w in enumerate(array[-1]):\n",
    "                if (misspellings == True):\n",
    "                    array[-1][i] = self.spell.correction(w)\n",
    "                array[-1][i] = lemmatizer.lemmatize(w)\n",
    "                    \n",
    "        if (misspellings):\n",
    "            self.tweets[\"WordNetLemmatizer+misspellings\"] = array\n",
    "        else:\n",
    "            self.tweets[\"WordNetLemmatizer\"] = array\n",
    "\n",
    "    def good_preproc(self):\n",
    "        array = []\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        nltk.download('stopwords')\n",
    "        english_stopwords = stopwords.words('english')\n",
    "\n",
    "        for sent in self.tweets.tweets:\n",
    "            array.append(word_tokenize(sent))\n",
    "            for i, w in enumerate(array[-1]):\n",
    "                array[-1][i] = porter_stemmer.stem(w)\n",
    "                if (len(array[-1][i]) < 3):\n",
    "                    array[-1].pop(i)\n",
    "                elif (array[-1][i] in english_stopwords):\n",
    "                    array[-1].pop(i)\n",
    "                elif (array[-1][i].isdigit()):\n",
    "                    array[-1].pop(i)\n",
    "        self.tweets[\"good_preproc\"] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/m1zdnv7157ncsc484c8h7fvr0000gn/T/ipykernel_33846/1742941878.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets = tweets.append(df)\n",
      "/var/folders/cc/m1zdnv7157ncsc484c8h7fvr0000gn/T/ipykernel_33846/1742941878.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets = tweets.append(df)\n",
      "/var/folders/cc/m1zdnv7157ncsc484c8h7fvr0000gn/T/ipykernel_33846/1742941878.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tweets = tweets.append(df)\n",
      "[nltk_data] Downloading package wordnet to /Users/banji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/banji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/banji/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#минут 6 учицца - норма\n",
    "\n",
    "preproc = Preprocessing(\"./data\")\n",
    "preproc.tokenization()\n",
    "preproc.PorterStemmer(False)\n",
    "preproc.WordNetLemmatizer(False)\n",
    "preproc.WordNetLemmatizer(True)\n",
    "preproc.PorterStemmer(True)\n",
    "preproc.good_preproc()\n",
    "preproc_data = preproc.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>category</th>\n",
       "      <th>just_tokenization</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>WordNetLemmatizer</th>\n",
       "      <th>WordNetLemmatizer+misspellings</th>\n",
       "      <th>PorterStemmer+misspellings</th>\n",
       "      <th>good_preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How unhappy  some dogs like it though</td>\n",
       "      <td>-1</td>\n",
       "      <td>[How, unhappy, some, dogs, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[How, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[How, unhappy, some, dog, like, it, though]</td>\n",
       "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
       "      <td>[unhappy, dogs, like, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talking to my over driver about where I'm goin...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
       "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
       "      <td>[talk, my, driver, where, 'm, goingh, said, 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[Does, anybody, know, if, the, Rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[Does, anybody, know, if, the, Rand, 's, likel...</td>\n",
       "      <td>[Does, anybody, know, if, the, Rand, 's, likel...</td>\n",
       "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
       "      <td>[doe, anybodi, know, the, rand, likely, fall, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
       "      <td>-1</td>\n",
       "      <td>[I, miss, going, to, gigs, in, Liverpool, unha...</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[I, miss, going, to, gig, in, Liverpool, unhappy]</td>\n",
       "      <td>[I, miss, going, to, gig, in, Liverpool, unhappy]</td>\n",
       "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
       "      <td>[miss, to, gig, Liverpool, unhappi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
       "      <td>-1</td>\n",
       "      <td>[There, isnt, a, new, Riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[There, isnt, a, new, Riverdale, tonight, ?, u...</td>\n",
       "      <td>[There, isnt, a, new, Riverdale, tonight, ?, u...</td>\n",
       "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
       "      <td>[isnt, new, riverdal, tonight, unhappy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>Supreme Court shoots down govt bid to put spor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Supreme, Court, shoots, down, govt, bid, to, ...</td>\n",
       "      <td>[suprem, court, shoot, down, govt, bid, to, pu...</td>\n",
       "      <td>[Supreme, Court, shoot, down, govt, bid, to, p...</td>\n",
       "      <td>[Supreme, Court, shoot, down, govt, bid, to, p...</td>\n",
       "      <td>[suprem, court, shoot, down, govt, bid, to, pu...</td>\n",
       "      <td>[suprem, court, shoot, govt, bid, put, sport, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>Historian Ram Guha, IDFC official Vikram Limay...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Historian, Ram, Guha, ,, IDFC, official, Vikr...</td>\n",
       "      <td>[historian, ram, guha, ,, idfc, offici, vikram...</td>\n",
       "      <td>[Historian, Ram, Guha, ,, IDFC, official, Vikr...</td>\n",
       "      <td>[Historian, Ram, Guha, ,, IDFC, official, Vikr...</td>\n",
       "      <td>[historian, ram, guha, ,, idfc, offici, vikram...</td>\n",
       "      <td>[historian, ram, guha, IDFC, offici, vikram, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Supreme, Court, names, former, CAG, as, head,...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[Supreme, Court, name, former, CAG, a, head, o...</td>\n",
       "      <td>[Supreme, Court, name, former, CAG, a, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
       "      <td>[suprem, court, name, former, cag, head, 4-mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Court, summons, CM, suspended, BJP, MP, as, a...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[Court, summons, CM, suspended, BJP, MP, a, ac...</td>\n",
       "      <td>[Court, summons, CM, suspended, BJP, MP, a, ac...</td>\n",
       "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
       "      <td>[court, summon, suspended, bjp, as, accus, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Amulya, Patnaik, has, been, appointed, new, D...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[Amulya, Patnaik, ha, been, appointed, new, De...</td>\n",
       "      <td>[Amulya, Patnaik, ha, been, appointed, new, De...</td>\n",
       "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
       "      <td>[amulya, patnaik, been, appoint, new, delhi, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2745 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets  category  \\\n",
       "0                 How unhappy  some dogs like it though        -1   \n",
       "1     talking to my over driver about where I'm goin...        -1   \n",
       "2     Does anybody know if the Rand's likely to fall...        -1   \n",
       "3            I miss going to gigs in Liverpool unhappy         -1   \n",
       "4         There isnt a new Riverdale tonight ? unhappy         -1   \n",
       "...                                                 ...       ...   \n",
       "2740  Supreme Court shoots down govt bid to put spor...         0   \n",
       "2741  Historian Ram Guha, IDFC official Vikram Limay...         0   \n",
       "2742  Supreme Court names former CAG as head of 4-me...         0   \n",
       "2743  Court summons CM suspended BJP MP as accused i...         0   \n",
       "2744  Amulya Patnaik has been appointed new Delhi po...         0   \n",
       "\n",
       "                                      just_tokenization  \\\n",
       "0          [How, unhappy, some, dogs, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [Does, anybody, know, if, the, Rand, 's, likel...   \n",
       "3     [I, miss, going, to, gigs, in, Liverpool, unha...   \n",
       "4     [There, isnt, a, new, Riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "2740  [Supreme, Court, shoots, down, govt, bid, to, ...   \n",
       "2741  [Historian, Ram, Guha, ,, IDFC, official, Vikr...   \n",
       "2742  [Supreme, Court, names, former, CAG, as, head,...   \n",
       "2743  [Court, summons, CM, suspended, BJP, MP, as, a...   \n",
       "2744  [Amulya, Patnaik, has, been, appointed, new, D...   \n",
       "\n",
       "                                          PorterStemmer  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "2740  [suprem, court, shoot, down, govt, bid, to, pu...   \n",
       "2741  [historian, ram, guha, ,, idfc, offici, vikram...   \n",
       "2742  [suprem, court, name, former, cag, as, head, o...   \n",
       "2743  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "2744  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                      WordNetLemmatizer  \\\n",
       "0           [How, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [Does, anybody, know, if, the, Rand, 's, likel...   \n",
       "3     [I, miss, going, to, gig, in, Liverpool, unhappy]   \n",
       "4     [There, isnt, a, new, Riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "2740  [Supreme, Court, shoot, down, govt, bid, to, p...   \n",
       "2741  [Historian, Ram, Guha, ,, IDFC, official, Vikr...   \n",
       "2742  [Supreme, Court, name, former, CAG, a, head, o...   \n",
       "2743  [Court, summons, CM, suspended, BJP, MP, a, ac...   \n",
       "2744  [Amulya, Patnaik, ha, been, appointed, new, De...   \n",
       "\n",
       "                         WordNetLemmatizer+misspellings  \\\n",
       "0           [How, unhappy, some, dog, like, it, though]   \n",
       "1     [talking, to, my, over, driver, about, where, ...   \n",
       "2     [Does, anybody, know, if, the, Rand, 's, likel...   \n",
       "3     [I, miss, going, to, gig, in, Liverpool, unhappy]   \n",
       "4     [There, isnt, a, new, Riverdale, tonight, ?, u...   \n",
       "...                                                 ...   \n",
       "2740  [Supreme, Court, shoot, down, govt, bid, to, p...   \n",
       "2741  [Historian, Ram, Guha, ,, IDFC, official, Vikr...   \n",
       "2742  [Supreme, Court, name, former, CAG, a, head, o...   \n",
       "2743  [Court, summons, CM, suspended, BJP, MP, a, ac...   \n",
       "2744  [Amulya, Patnaik, ha, been, appointed, new, De...   \n",
       "\n",
       "                             PorterStemmer+misspellings  \\\n",
       "0           [how, unhappi, some, dog, like, it, though]   \n",
       "1     [talk, to, my, over, driver, about, where, i, ...   \n",
       "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
       "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
       "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
       "...                                                 ...   \n",
       "2740  [suprem, court, shoot, down, govt, bid, to, pu...   \n",
       "2741  [historian, ram, guha, ,, idfc, offici, vikram...   \n",
       "2742  [suprem, court, name, former, cag, as, head, o...   \n",
       "2743  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
       "2744  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
       "\n",
       "                                           good_preproc  \n",
       "0                         [unhappy, dogs, like, though]  \n",
       "1     [talk, my, driver, where, 'm, goingh, said, 'd...  \n",
       "2     [doe, anybodi, know, the, rand, likely, fall, ...  \n",
       "3                   [miss, to, gig, Liverpool, unhappi]  \n",
       "4               [isnt, new, riverdal, tonight, unhappy]  \n",
       "...                                                 ...  \n",
       "2740  [suprem, court, shoot, govt, bid, put, sport, ...  \n",
       "2741  [historian, ram, guha, IDFC, offici, vikram, l...  \n",
       "2742  [suprem, court, name, former, cag, head, 4-mem...  \n",
       "2743  [court, summon, suspended, bjp, as, accus, cri...  \n",
       "2744  [amulya, patnaik, been, appoint, new, delhi, p...  \n",
       "\n",
       "[2745 rows x 8 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1048\n",
       "-1     859\n",
       " 1     838\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TO_VEC:\n",
    "    def __init__(self, data_frame):\n",
    "        self.text = data_frame\n",
    "        self.vectors = pd.DataFrame()\n",
    "        self.vectorizers = {}\n",
    "\n",
    "    def to_bin_counts(self):\n",
    "        coun_vect = CountVectorizer(binary=True)\n",
    "        for i in range(2, self.text.shape[1]):\n",
    "            text = pd.DataFrame(map(lambda x: ' '.join(x), self.text.iloc[:,2])).iloc[:, 0].tolist()\n",
    "            name = self.text.iloc[:,i].name\n",
    "            count_matrix = coun_vect.fit_transform(text)\n",
    "            count_array = count_matrix.toarray()\n",
    "            self.vectors[\"bin_vec_{}\".format(name)] = list(count_array)\n",
    "            self.vectorizers[\"bin_vec_{}\".format(name)] = coun_vect\n",
    "\n",
    "    def to_TFIDF(self):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        for i in range(2, self.text.shape[1]):\n",
    "            text = pd.DataFrame(map(lambda x: ' '.join(x), self.text.iloc[:,i])).iloc[:, 0].tolist()\n",
    "            name = self.text.iloc[:,i].name\n",
    "            count_matrix = vectorizer.fit_transform(text)\n",
    "            count_array = count_matrix.toarray()\n",
    "            self.vectors[\"TFIDF_{}\".format(name)] = list(count_array)\n",
    "            self.vectorizers[\"TFIDF_{}\".format(name)] = vectorizer\n",
    "\n",
    "    def to_word_counts(self):\n",
    "        coun_vect = CountVectorizer()\n",
    "        for i in range(2, self.text.shape[1]):\n",
    "            text = pd.DataFrame(map(lambda x: ' '.join(x), self.text.iloc[:,i])).iloc[:, 0].tolist()\n",
    "            name = self.text.iloc[:,i].name\n",
    "            count_matrix = coun_vect.fit_transform(text)\n",
    "            count_array = count_matrix.toarray()\n",
    "            self.vectors[\"word_counts_{}\".format(name)] = list(count_array)\n",
    "            self.vectorizers[\"word_counts_{}\".format(name)] = coun_vect\n",
    "\n",
    "\n",
    "    # def to_wordtovec:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = TO_VEC(preproc_data)\n",
    "vectors.to_bin_counts()\n",
    "vectors.to_TFIDF()\n",
    "vectors.to_word_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_vec_just_tokenization</th>\n",
       "      <th>bin_vec_PorterStemmer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
       "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
       "      <th>bin_vec_good_preproc</th>\n",
       "      <th>TFIDF_just_tokenization</th>\n",
       "      <th>TFIDF_PorterStemmer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
       "      <th>TFIDF_PorterStemmer+misspellings</th>\n",
       "      <th>TFIDF_good_preproc</th>\n",
       "      <th>word_counts_just_tokenization</th>\n",
       "      <th>word_counts_PorterStemmer</th>\n",
       "      <th>word_counts_WordNetLemmatizer</th>\n",
       "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
       "      <th>word_counts_PorterStemmer+misspellings</th>\n",
       "      <th>word_counts_good_preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2742</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2745 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              bin_vec_just_tokenization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  bin_vec_PorterStemmer  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                              bin_vec_WordNetLemmatizer  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                 bin_vec_WordNetLemmatizer+misspellings  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                     bin_vec_PorterStemmer+misspellings  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                   bin_vec_good_preproc  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                TFIDF_just_tokenization  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                    TFIDF_PorterStemmer  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                TFIDF_WordNetLemmatizer  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                   TFIDF_WordNetLemmatizer+misspellings  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                       TFIDF_PorterStemmer+misspellings  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                     TFIDF_good_preproc  \\\n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                          word_counts_just_tokenization  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                              word_counts_PorterStemmer  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                          word_counts_WordNetLemmatizer  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "             word_counts_WordNetLemmatizer+misspellings  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                 word_counts_PorterStemmer+misspellings  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                               word_counts_good_preproc  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[2745 rows x 18 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, target):\n",
    "    accuracy_table = pd.DataFrame(columns=list(data.keys()))\n",
    "    for i in range(data.shape[1]):\n",
    "        X = data.iloc[:,i]\n",
    "        y = target\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.8, \n",
    "                                                    random_state=42,\n",
    "                                                    shuffle = True)\n",
    "        X_train = pd.DataFrame(X_train.tolist())\n",
    "        X_test = pd.DataFrame(X_test.tolist())\n",
    "        y_train = y_train.tolist()\n",
    "        y_test = y_test.tolist()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy_table.iloc[:,i] = [accuracy_score(predict, y_test)]\n",
    "        \n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "accuracy_table_svm = train(svc, vectors.vectors, preproc_data.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_vec_just_tokenization</th>\n",
       "      <th>bin_vec_PorterStemmer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
       "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
       "      <th>bin_vec_good_preproc</th>\n",
       "      <th>TFIDF_just_tokenization</th>\n",
       "      <th>TFIDF_PorterStemmer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
       "      <th>TFIDF_PorterStemmer+misspellings</th>\n",
       "      <th>TFIDF_good_preproc</th>\n",
       "      <th>word_counts_just_tokenization</th>\n",
       "      <th>word_counts_PorterStemmer</th>\n",
       "      <th>word_counts_WordNetLemmatizer</th>\n",
       "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
       "      <th>word_counts_PorterStemmer+misspellings</th>\n",
       "      <th>word_counts_good_preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.907104</td>\n",
       "      <td>0.897996</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.905282</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.905282</td>\n",
       "      <td>0.897996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_vec_just_tokenization  bin_vec_PorterStemmer  \\\n",
       "0                   0.907104               0.907104   \n",
       "\n",
       "   bin_vec_WordNetLemmatizer  bin_vec_WordNetLemmatizer+misspellings  \\\n",
       "0                   0.907104                                0.907104   \n",
       "\n",
       "   bin_vec_PorterStemmer+misspellings  bin_vec_good_preproc  \\\n",
       "0                            0.907104              0.907104   \n",
       "\n",
       "   TFIDF_just_tokenization  TFIDF_PorterStemmer  TFIDF_WordNetLemmatizer  \\\n",
       "0                 0.899818             0.907104                 0.901639   \n",
       "\n",
       "   TFIDF_WordNetLemmatizer+misspellings  TFIDF_PorterStemmer+misspellings  \\\n",
       "0                              0.901639                          0.907104   \n",
       "\n",
       "   TFIDF_good_preproc  word_counts_just_tokenization  \\\n",
       "0            0.897996                       0.901639   \n",
       "\n",
       "   word_counts_PorterStemmer  word_counts_WordNetLemmatizer  \\\n",
       "0                   0.905282                       0.903461   \n",
       "\n",
       "   word_counts_WordNetLemmatizer+misspellings  \\\n",
       "0                                    0.903461   \n",
       "\n",
       "   word_counts_PorterStemmer+misspellings  word_counts_good_preproc  \n",
       "0                                0.905282                  0.897996  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_table_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "accuracy_table_logreg = train(lr, vectors.vectors, preproc_data.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_vec_just_tokenization</th>\n",
       "      <th>bin_vec_PorterStemmer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
       "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
       "      <th>bin_vec_good_preproc</th>\n",
       "      <th>TFIDF_just_tokenization</th>\n",
       "      <th>TFIDF_PorterStemmer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
       "      <th>TFIDF_PorterStemmer+misspellings</th>\n",
       "      <th>TFIDF_good_preproc</th>\n",
       "      <th>word_counts_just_tokenization</th>\n",
       "      <th>word_counts_PorterStemmer</th>\n",
       "      <th>word_counts_WordNetLemmatizer</th>\n",
       "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
       "      <th>word_counts_PorterStemmer+misspellings</th>\n",
       "      <th>word_counts_good_preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.910747</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.908925</td>\n",
       "      <td>0.908925</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.901639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_vec_just_tokenization  bin_vec_PorterStemmer  \\\n",
       "0                   0.903461               0.903461   \n",
       "\n",
       "   bin_vec_WordNetLemmatizer  bin_vec_WordNetLemmatizer+misspellings  \\\n",
       "0                   0.903461                                0.903461   \n",
       "\n",
       "   bin_vec_PorterStemmer+misspellings  bin_vec_good_preproc  \\\n",
       "0                            0.903461              0.903461   \n",
       "\n",
       "   TFIDF_just_tokenization  TFIDF_PorterStemmer  TFIDF_WordNetLemmatizer  \\\n",
       "0                 0.910747             0.918033                 0.908925   \n",
       "\n",
       "   TFIDF_WordNetLemmatizer+misspellings  TFIDF_PorterStemmer+misspellings  \\\n",
       "0                              0.908925                          0.918033   \n",
       "\n",
       "   TFIDF_good_preproc  word_counts_just_tokenization  \\\n",
       "0            0.899818                       0.899818   \n",
       "\n",
       "   word_counts_PorterStemmer  word_counts_WordNetLemmatizer  \\\n",
       "0                   0.901639                       0.901639   \n",
       "\n",
       "   word_counts_WordNetLemmatizer+misspellings  \\\n",
       "0                                    0.901639   \n",
       "\n",
       "   word_counts_PorterStemmer+misspellings  word_counts_good_preproc  \n",
       "0                                0.901639                  0.901639  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_table_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "# parameters = {'max_depth':[3,10]} #лучший 10\n",
    "# gsCV = GridSearchCV(gbc, parameters)\n",
    "accuracy_table_gbc = train(gbc, vectors.vectors, preproc_data.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_vec_just_tokenization</th>\n",
       "      <th>bin_vec_PorterStemmer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer</th>\n",
       "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
       "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
       "      <th>bin_vec_good_preproc</th>\n",
       "      <th>TFIDF_just_tokenization</th>\n",
       "      <th>TFIDF_PorterStemmer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer</th>\n",
       "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
       "      <th>TFIDF_PorterStemmer+misspellings</th>\n",
       "      <th>TFIDF_good_preproc</th>\n",
       "      <th>word_counts_just_tokenization</th>\n",
       "      <th>word_counts_PorterStemmer</th>\n",
       "      <th>word_counts_WordNetLemmatizer</th>\n",
       "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
       "      <th>word_counts_PorterStemmer+misspellings</th>\n",
       "      <th>word_counts_good_preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.896175</td>\n",
       "      <td>0.897996</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.897996</td>\n",
       "      <td>0.897996</td>\n",
       "      <td>0.896175</td>\n",
       "      <td>0.896175</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.897996</td>\n",
       "      <td>0.894353</td>\n",
       "      <td>0.89071</td>\n",
       "      <td>0.896175</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.903461</td>\n",
       "      <td>0.899818</td>\n",
       "      <td>0.89071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_vec_just_tokenization  bin_vec_PorterStemmer  \\\n",
       "0                   0.896175               0.897996   \n",
       "\n",
       "   bin_vec_WordNetLemmatizer  bin_vec_WordNetLemmatizer+misspellings  \\\n",
       "0                   0.901639                                0.903461   \n",
       "\n",
       "   bin_vec_PorterStemmer+misspellings  bin_vec_good_preproc  \\\n",
       "0                            0.897996              0.897996   \n",
       "\n",
       "   TFIDF_just_tokenization  TFIDF_PorterStemmer  TFIDF_WordNetLemmatizer  \\\n",
       "0                 0.896175             0.896175                 0.899818   \n",
       "\n",
       "   TFIDF_WordNetLemmatizer+misspellings  TFIDF_PorterStemmer+misspellings  \\\n",
       "0                              0.897996                          0.894353   \n",
       "\n",
       "   TFIDF_good_preproc  word_counts_just_tokenization  \\\n",
       "0             0.89071                       0.896175   \n",
       "\n",
       "   word_counts_PorterStemmer  word_counts_WordNetLemmatizer  \\\n",
       "0                   0.899818                       0.899818   \n",
       "\n",
       "   word_counts_WordNetLemmatizer+misspellings  \\\n",
       "0                                    0.903461   \n",
       "\n",
       "   word_counts_PorterStemmer+misspellings  word_counts_good_preproc  \n",
       "0                                0.899818                   0.89071  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_table_gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top-10 most similar pair of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[159, 214, 1.0000000000000002],\n",
       " [179, 851, 1.0000000000000002],\n",
       " [214, 159, 1.0000000000000002],\n",
       " [437, 711, 1.0000000000000002],\n",
       " [574, 159, 1.0000000000000002],\n",
       " [608, 679, 1.0000000000000002],\n",
       " [654, 690, 1.0000000000000002],\n",
       " [679, 608, 1.0000000000000002],\n",
       " [690, 654, 1.0000000000000002],\n",
       " [711, 437, 1.0000000000000002]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "matrix_similarity = []\n",
    "data_1 = vectors.vectors.TFIDF_good_preproc\n",
    "data_2 = vectors.vectors.TFIDF_WordNetLemmatizer\n",
    "\n",
    "matrix_similarity = cosine_similarity(list(data_1))\n",
    "cos_sim = []\n",
    "for i in range(len(matrix_similarity)):\n",
    "    for j in range(len(matrix_similarity[i])):\n",
    "        if (i == j): matrix_similarity[i][j] = -np.inf\n",
    "    a = list([i, np.argmax(matrix_similarity[i]),max(matrix_similarity[i])])\n",
    "    cos_sim.append(a)\n",
    "\n",
    "cos_sim.sort(key = lambda x: -x[2])\n",
    "\n",
    "cos_sim[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I miss you unhappy ', 'i miss them unhappy ')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.text.tweets[159], vectors.text.tweets[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('babies unhappy  ', 'baby unhappy  ')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.text.tweets[711], vectors.text.tweets[437]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(text, model, vectorizer):\n",
    "    vectors = vectorizer.transform(text)\n",
    "    return model.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_class([\"i love you\", \"fuck\", 'abra abrab ab', 'bad bad bad bad bad', 'thank you very much you helped me'], gbc, vectors.vectorizers['TFIDF_good_preproc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot_conf_matrix(true, pred, cat=['-1', '0', '1']):\n",
    "#     sns.heatmap(data=confusion_matrix(true, pred), \n",
    "#             annot=True, fmt=\"d\", cbar=False, xticklabels=cat, yticklabels=cat)\n",
    "\n",
    "\n",
    "# plot_conf_matrix(y_test, tfidf_pred_data)\n",
    "\n",
    "# accuracy_score(y_test, tfidf_pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#понадобится для nltk.download()\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
