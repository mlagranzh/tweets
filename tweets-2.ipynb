{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHTb18ChXNOO",
        "outputId": "d326a58e-dc49-4d69-8008-1b8a3ff54364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyspellchecker in /Users/banji/Library/Python/3.8/lib/python/site-packages (0.6.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in /Users/banji/Library/Python/3.8/lib/python/site-packages (3.7)\n",
            "Requirement already satisfied: click in /Users/banji/Library/Python/3.8/lib/python/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /Users/banji/Library/Python/3.8/lib/python/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/banji/Library/Python/3.8/lib/python/site-packages (from nltk) (2022.4.24)\n",
            "Requirement already satisfied: joblib in /Users/banji/Library/Python/3.8/lib/python/site-packages (from nltk) (1.1.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gensim in /Users/banji/Library/Python/3.8/lib/python/site-packages (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/banji/Library/Python/3.8/lib/python/site-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/banji/Library/Python/3.8/lib/python/site-packages (from gensim) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/banji/Library/Python/3.8/lib/python/site-packages (from gensim) (1.8.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/banji/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip3 install pyspellchecker\n",
        "!pip3 install nltk\n",
        "!pip3 install gensim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import warnings\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1l0fCyXNOT",
        "outputId": "c057b1d7-7608-4b9f-a9dd-ce74730d80a1"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, path_to_data):\n",
        "        files = []\n",
        "        tweets = pd.DataFrame()\n",
        "        for filenames in os.listdir(path_to_data):\n",
        "            if re.findall('.+\\.csv', filenames):\n",
        "                files.append(filenames)\n",
        "        for file in files:\n",
        "            with open(path_to_data + '/' + file, 'r') as f:\n",
        "                data = f.read()\n",
        "            data = re.sub(r',(?=[^ ])', '--', data)\n",
        "            df = pd.DataFrame(data.split('--'), columns=['tweets'])\n",
        "            df['category'] = file\n",
        "            df.drop_duplicates(inplace=True)\n",
        "            tweets = tweets.append(df)\n",
        "        tweets['category'].replace({\n",
        "            'processedNegative.csv' : -1, \n",
        "            'processedNeutral.csv': 0, \n",
        "            'processedPositive.csv' : 1}, inplace=True)\n",
        "        tweets.index = np.arange(len(tweets))\n",
        "        self.tweets = tweets\n",
        "        self.spell = SpellChecker()\n",
        "\n",
        "    def tokenization(self):\n",
        "        array = []\n",
        "        for sent in self.tweets.tweets:\n",
        "            array.append(word_tokenize(sent))\n",
        "        self.tweets[\"just_tokenization\"] = array\n",
        "\n",
        "    def PorterStemmer(self, misspellings=False):\n",
        "        array = []\n",
        "        porter_stemmer = PorterStemmer()\n",
        "        for sent in self.tweets.tweets:\n",
        "            array.append(word_tokenize(sent))\n",
        "            for i, w in enumerate(array[-1]):\n",
        "                if (misspellings == True):\n",
        "                    array[-1][i] = self.spell.correction(w)\n",
        "                array[-1][i] = porter_stemmer.stem(w)\n",
        "    \n",
        "        if (misspellings):\n",
        "            self.tweets[\"PorterStemmer+misspellings\"] = array\n",
        "        else:\n",
        "            self.tweets[\"PorterStemmer\"] = array\n",
        "\n",
        "    def WordNetLemmatizer(self, misspellings=False):\n",
        "        nltk.download('wordnet')\n",
        "        array = []\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        for sent in self.tweets.tweets:\n",
        "            array.append(word_tokenize(sent))\n",
        "            for i, w in enumerate(array[-1]):\n",
        "                if (misspellings == True):\n",
        "                    array[-1][i] = self.spell.correction(w)\n",
        "                array[-1][i] = lemmatizer.lemmatize(w)\n",
        "                    \n",
        "        if (misspellings):\n",
        "            self.tweets[\"WordNetLemmatizer+misspellings\"] = array\n",
        "        else:\n",
        "            self.tweets[\"WordNetLemmatizer\"] = array\n",
        "\n",
        "    def good_preproc(self):\n",
        "        array = []\n",
        "        porter_stemmer = PorterStemmer()\n",
        "        nltk.download('stopwords')\n",
        "        english_stopwords = stopwords.words('english')\n",
        "\n",
        "        for sent in self.tweets.tweets:\n",
        "            array.append(word_tokenize(sent))\n",
        "            for i, w in enumerate(array[-1]):\n",
        "                array[-1][i] = porter_stemmer.stem(w)\n",
        "                if (len(array[-1][i]) < 3):\n",
        "                    array[-1].pop(i)\n",
        "                elif (array[-1][i] in english_stopwords):\n",
        "                    array[-1].pop(i)\n",
        "                elif (array[-1][i].isdigit()):\n",
        "                    array[-1].pop(i)\n",
        "        self.tweets[\"good_preproc\"] = array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "RXDDWfhaXNOW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/cc/m1zdnv7157ncsc484c8h7fvr0000gn/T/ipykernel_19423/1090301464.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tweets = tweets.append(df)\n",
            "/var/folders/cc/m1zdnv7157ncsc484c8h7fvr0000gn/T/ipykernel_19423/1090301464.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tweets = tweets.append(df)\n",
            "/var/folders/cc/m1zdnv7157ncsc484c8h7fvr0000gn/T/ipykernel_19423/1090301464.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  tweets = tweets.append(df)\n",
            "[nltk_data] Downloading package wordnet to /Users/banji/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/banji/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/banji/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#минут 6 учицца - норма\n",
        "\n",
        "preproc = Preprocessing(\"./data\")\n",
        "preproc.tokenization()\n",
        "preproc.PorterStemmer(False)\n",
        "preproc.WordNetLemmatizer(False)\n",
        "preproc.WordNetLemmatizer(True)\n",
        "preproc.PorterStemmer(True)\n",
        "preproc.good_preproc()\n",
        "preproc_data = preproc.tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "6_PpRDh7XNOY",
        "outputId": "4e2068e6-7c71-4902-ffa0-4d8984e5d477"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>category</th>\n",
              "      <th>just_tokenization</th>\n",
              "      <th>PorterStemmer</th>\n",
              "      <th>WordNetLemmatizer</th>\n",
              "      <th>WordNetLemmatizer+misspellings</th>\n",
              "      <th>PorterStemmer+misspellings</th>\n",
              "      <th>good_preproc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How unhappy  some dogs like it though</td>\n",
              "      <td>-1</td>\n",
              "      <td>[How, unhappy, some, dogs, like, it, though]</td>\n",
              "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
              "      <td>[How, unhappy, some, dog, like, it, though]</td>\n",
              "      <td>[How, unhappy, some, dog, like, it, though]</td>\n",
              "      <td>[how, unhappi, some, dog, like, it, though]</td>\n",
              "      <td>[unhappy, dogs, like, though]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talking to my over driver about where I'm goin...</td>\n",
              "      <td>-1</td>\n",
              "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
              "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
              "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
              "      <td>[talking, to, my, over, driver, about, where, ...</td>\n",
              "      <td>[talk, to, my, over, driver, about, where, i, ...</td>\n",
              "      <td>[talk, my, driver, where, 'm, goingh, said, 'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Does anybody know if the Rand's likely to fall...</td>\n",
              "      <td>-1</td>\n",
              "      <td>[Does, anybody, know, if, the, Rand, 's, likel...</td>\n",
              "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
              "      <td>[Does, anybody, know, if, the, Rand, 's, likel...</td>\n",
              "      <td>[Does, anybody, know, if, the, Rand, 's, likel...</td>\n",
              "      <td>[doe, anybodi, know, if, the, rand, 's, like, ...</td>\n",
              "      <td>[doe, anybodi, know, the, rand, likely, fall, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I miss going to gigs in Liverpool unhappy</td>\n",
              "      <td>-1</td>\n",
              "      <td>[I, miss, going, to, gigs, in, Liverpool, unha...</td>\n",
              "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
              "      <td>[I, miss, going, to, gig, in, Liverpool, unhappy]</td>\n",
              "      <td>[I, miss, going, to, gig, in, Liverpool, unhappy]</td>\n",
              "      <td>[i, miss, go, to, gig, in, liverpool, unhappi]</td>\n",
              "      <td>[miss, to, gig, Liverpool, unhappi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>There isnt a new Riverdale tonight ? unhappy</td>\n",
              "      <td>-1</td>\n",
              "      <td>[There, isnt, a, new, Riverdale, tonight, ?, u...</td>\n",
              "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
              "      <td>[There, isnt, a, new, Riverdale, tonight, ?, u...</td>\n",
              "      <td>[There, isnt, a, new, Riverdale, tonight, ?, u...</td>\n",
              "      <td>[there, isnt, a, new, riverdal, tonight, ?, un...</td>\n",
              "      <td>[isnt, new, riverdal, tonight, unhappy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2740</th>\n",
              "      <td>Supreme Court shoots down govt bid to put spor...</td>\n",
              "      <td>0</td>\n",
              "      <td>[Supreme, Court, shoots, down, govt, bid, to, ...</td>\n",
              "      <td>[suprem, court, shoot, down, govt, bid, to, pu...</td>\n",
              "      <td>[Supreme, Court, shoot, down, govt, bid, to, p...</td>\n",
              "      <td>[Supreme, Court, shoot, down, govt, bid, to, p...</td>\n",
              "      <td>[suprem, court, shoot, down, govt, bid, to, pu...</td>\n",
              "      <td>[suprem, court, shoot, govt, bid, put, sport, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2741</th>\n",
              "      <td>Historian Ram Guha, IDFC official Vikram Limay...</td>\n",
              "      <td>0</td>\n",
              "      <td>[Historian, Ram, Guha, ,, IDFC, official, Vikr...</td>\n",
              "      <td>[historian, ram, guha, ,, idfc, offici, vikram...</td>\n",
              "      <td>[Historian, Ram, Guha, ,, IDFC, official, Vikr...</td>\n",
              "      <td>[Historian, Ram, Guha, ,, IDFC, official, Vikr...</td>\n",
              "      <td>[historian, ram, guha, ,, idfc, offici, vikram...</td>\n",
              "      <td>[historian, ram, guha, IDFC, offici, vikram, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2742</th>\n",
              "      <td>Supreme Court names former CAG as head of 4-me...</td>\n",
              "      <td>0</td>\n",
              "      <td>[Supreme, Court, names, former, CAG, as, head,...</td>\n",
              "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
              "      <td>[Supreme, Court, name, former, CAG, a, head, o...</td>\n",
              "      <td>[Supreme, Court, name, former, CAG, a, head, o...</td>\n",
              "      <td>[suprem, court, name, former, cag, as, head, o...</td>\n",
              "      <td>[suprem, court, name, former, cag, head, 4-mem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2743</th>\n",
              "      <td>Court summons CM suspended BJP MP as accused i...</td>\n",
              "      <td>0</td>\n",
              "      <td>[Court, summons, CM, suspended, BJP, MP, as, a...</td>\n",
              "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
              "      <td>[Court, summons, CM, suspended, BJP, MP, a, ac...</td>\n",
              "      <td>[Court, summons, CM, suspended, BJP, MP, a, ac...</td>\n",
              "      <td>[court, summon, cm, suspend, bjp, mp, as, accu...</td>\n",
              "      <td>[court, summon, suspended, bjp, as, accus, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2744</th>\n",
              "      <td>Amulya Patnaik has been appointed new Delhi po...</td>\n",
              "      <td>0</td>\n",
              "      <td>[Amulya, Patnaik, has, been, appointed, new, D...</td>\n",
              "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
              "      <td>[Amulya, Patnaik, ha, been, appointed, new, De...</td>\n",
              "      <td>[Amulya, Patnaik, ha, been, appointed, new, De...</td>\n",
              "      <td>[amulya, patnaik, ha, been, appoint, new, delh...</td>\n",
              "      <td>[amulya, patnaik, been, appoint, new, delhi, p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2745 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  category  \\\n",
              "0                 How unhappy  some dogs like it though        -1   \n",
              "1     talking to my over driver about where I'm goin...        -1   \n",
              "2     Does anybody know if the Rand's likely to fall...        -1   \n",
              "3            I miss going to gigs in Liverpool unhappy         -1   \n",
              "4         There isnt a new Riverdale tonight ? unhappy         -1   \n",
              "...                                                 ...       ...   \n",
              "2740  Supreme Court shoots down govt bid to put spor...         0   \n",
              "2741  Historian Ram Guha, IDFC official Vikram Limay...         0   \n",
              "2742  Supreme Court names former CAG as head of 4-me...         0   \n",
              "2743  Court summons CM suspended BJP MP as accused i...         0   \n",
              "2744  Amulya Patnaik has been appointed new Delhi po...         0   \n",
              "\n",
              "                                      just_tokenization  \\\n",
              "0          [How, unhappy, some, dogs, like, it, though]   \n",
              "1     [talking, to, my, over, driver, about, where, ...   \n",
              "2     [Does, anybody, know, if, the, Rand, 's, likel...   \n",
              "3     [I, miss, going, to, gigs, in, Liverpool, unha...   \n",
              "4     [There, isnt, a, new, Riverdale, tonight, ?, u...   \n",
              "...                                                 ...   \n",
              "2740  [Supreme, Court, shoots, down, govt, bid, to, ...   \n",
              "2741  [Historian, Ram, Guha, ,, IDFC, official, Vikr...   \n",
              "2742  [Supreme, Court, names, former, CAG, as, head,...   \n",
              "2743  [Court, summons, CM, suspended, BJP, MP, as, a...   \n",
              "2744  [Amulya, Patnaik, has, been, appointed, new, D...   \n",
              "\n",
              "                                          PorterStemmer  \\\n",
              "0           [how, unhappi, some, dog, like, it, though]   \n",
              "1     [talk, to, my, over, driver, about, where, i, ...   \n",
              "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
              "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
              "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
              "...                                                 ...   \n",
              "2740  [suprem, court, shoot, down, govt, bid, to, pu...   \n",
              "2741  [historian, ram, guha, ,, idfc, offici, vikram...   \n",
              "2742  [suprem, court, name, former, cag, as, head, o...   \n",
              "2743  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
              "2744  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
              "\n",
              "                                      WordNetLemmatizer  \\\n",
              "0           [How, unhappy, some, dog, like, it, though]   \n",
              "1     [talking, to, my, over, driver, about, where, ...   \n",
              "2     [Does, anybody, know, if, the, Rand, 's, likel...   \n",
              "3     [I, miss, going, to, gig, in, Liverpool, unhappy]   \n",
              "4     [There, isnt, a, new, Riverdale, tonight, ?, u...   \n",
              "...                                                 ...   \n",
              "2740  [Supreme, Court, shoot, down, govt, bid, to, p...   \n",
              "2741  [Historian, Ram, Guha, ,, IDFC, official, Vikr...   \n",
              "2742  [Supreme, Court, name, former, CAG, a, head, o...   \n",
              "2743  [Court, summons, CM, suspended, BJP, MP, a, ac...   \n",
              "2744  [Amulya, Patnaik, ha, been, appointed, new, De...   \n",
              "\n",
              "                         WordNetLemmatizer+misspellings  \\\n",
              "0           [How, unhappy, some, dog, like, it, though]   \n",
              "1     [talking, to, my, over, driver, about, where, ...   \n",
              "2     [Does, anybody, know, if, the, Rand, 's, likel...   \n",
              "3     [I, miss, going, to, gig, in, Liverpool, unhappy]   \n",
              "4     [There, isnt, a, new, Riverdale, tonight, ?, u...   \n",
              "...                                                 ...   \n",
              "2740  [Supreme, Court, shoot, down, govt, bid, to, p...   \n",
              "2741  [Historian, Ram, Guha, ,, IDFC, official, Vikr...   \n",
              "2742  [Supreme, Court, name, former, CAG, a, head, o...   \n",
              "2743  [Court, summons, CM, suspended, BJP, MP, a, ac...   \n",
              "2744  [Amulya, Patnaik, ha, been, appointed, new, De...   \n",
              "\n",
              "                             PorterStemmer+misspellings  \\\n",
              "0           [how, unhappi, some, dog, like, it, though]   \n",
              "1     [talk, to, my, over, driver, about, where, i, ...   \n",
              "2     [doe, anybodi, know, if, the, rand, 's, like, ...   \n",
              "3        [i, miss, go, to, gig, in, liverpool, unhappi]   \n",
              "4     [there, isnt, a, new, riverdal, tonight, ?, un...   \n",
              "...                                                 ...   \n",
              "2740  [suprem, court, shoot, down, govt, bid, to, pu...   \n",
              "2741  [historian, ram, guha, ,, idfc, offici, vikram...   \n",
              "2742  [suprem, court, name, former, cag, as, head, o...   \n",
              "2743  [court, summon, cm, suspend, bjp, mp, as, accu...   \n",
              "2744  [amulya, patnaik, ha, been, appoint, new, delh...   \n",
              "\n",
              "                                           good_preproc  \n",
              "0                         [unhappy, dogs, like, though]  \n",
              "1     [talk, my, driver, where, 'm, goingh, said, 'd...  \n",
              "2     [doe, anybodi, know, the, rand, likely, fall, ...  \n",
              "3                   [miss, to, gig, Liverpool, unhappi]  \n",
              "4               [isnt, new, riverdal, tonight, unhappy]  \n",
              "...                                                 ...  \n",
              "2740  [suprem, court, shoot, govt, bid, put, sport, ...  \n",
              "2741  [historian, ram, guha, IDFC, offici, vikram, l...  \n",
              "2742  [suprem, court, name, former, cag, head, 4-mem...  \n",
              "2743  [court, summon, suspended, bjp, as, accus, cri...  \n",
              "2744  [amulya, patnaik, been, appoint, new, delhi, p...  \n",
              "\n",
              "[2745 rows x 8 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preproc_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYszQ4h4XNOY",
        "outputId": "97b9b2dd-417b-4b1c-bb86-c9c96e2c546a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 0    1048\n",
              "-1     859\n",
              " 1     838\n",
              "Name: category, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preproc_data.category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "b_Oepij3XNOZ"
      },
      "outputs": [],
      "source": [
        "class TO_VEC:\n",
        "    def __init__(self, data_frame):\n",
        "        self.text = data_frame\n",
        "        self.vectors = pd.DataFrame()\n",
        "        self.vectorizers = {}\n",
        "\n",
        "    def to_bin_counts(self):\n",
        "        coun_vect = CountVectorizer(binary=True)\n",
        "        for i in range(2, self.text.shape[1]):\n",
        "            text = pd.DataFrame(map(lambda x: ' '.join(x), self.text.iloc[:,i])).iloc[:, 0].tolist()\n",
        "            name = self.text.iloc[:,i].name\n",
        "            count_matrix = coun_vect.fit_transform(text)\n",
        "            count_array = count_matrix.toarray()\n",
        "            self.vectors[\"bin_vec_{}\".format(name)] = list(count_array)\n",
        "            self.vectorizers[\"bin_vec_{}\".format(name)] = coun_vect\n",
        "\n",
        "    def to_TFIDF(self):\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        for i in range(2, self.text.shape[1]):\n",
        "            text = pd.DataFrame(map(lambda x: ' '.join(x), self.text.iloc[:,i])).iloc[:, 0].tolist()\n",
        "            name = self.text.iloc[:,i].name\n",
        "            count_matrix = vectorizer.fit_transform(text)\n",
        "            count_array = count_matrix.toarray()\n",
        "            self.vectors[\"TFIDF_{}\".format(name)] = list(count_array)\n",
        "            self.vectorizers[\"TFIDF_{}\".format(name)] = vectorizer\n",
        "\n",
        "    def to_word_counts(self):\n",
        "        coun_vect = CountVectorizer()\n",
        "        for i in range(2, self.text.shape[1]):\n",
        "            text = pd.DataFrame(map(lambda x: ' '.join(x), self.text.iloc[:,i])).iloc[:, 0].tolist()\n",
        "            name = self.text.iloc[:,i].name\n",
        "            count_matrix = coun_vect.fit_transform(text)\n",
        "            count_array = count_matrix.toarray()\n",
        "            self.vectors[\"word_counts_{}\".format(name)] = list(count_array)\n",
        "            self.vectorizers[\"word_counts_{}\".format(name)] = coun_vect\n",
        "\n",
        "\n",
        "    #Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data\n",
        "    def to_word2vec(self):\n",
        "      word2vec = gensim.downloader.load(\"glove-twitter-25\")\n",
        "      mean = np.mean(word2vec.vectors, 0)\n",
        "      std = np.std(word2vec.vectors, 0)\n",
        "      for i in range(2, self.text.shape[1]):\n",
        "        text = self.text.iloc[:,i].tolist()\n",
        "        array = []\n",
        "        name = self.text.iloc[:,i].name\n",
        "        for i in range(len(text)):\n",
        "          line = text[i]\n",
        "          filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
        "          embeddings = np.mean([(word2vec.get_vector(w) - mean)/std for w in filtered_line if w in word2vec],axis=0)\n",
        "          if len(embeddings.shape) == 0:\n",
        "            embeddings = np.zeros((1, word2vec.vector_size))\n",
        "          else:\n",
        "            embeddings = np.array(embeddings)\n",
        "            if len(embeddings.shape) == 1:\n",
        "              embeddings = embeddings.reshape(1, -1)\n",
        "          array.append(embeddings.reshape(25))\n",
        "        self.vectors[\"word2vec_{}\".format(name)] = array\n",
        "        # self.vectorizers[\"word2vec_{}\".format(name)] = coun_vect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYrEOEMHhPnq"
      },
      "source": [
        "Если хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDHhfrF8XNOa",
        "outputId": "d06b1407-e56a-4d18-bbad-d20af218409a"
      },
      "outputs": [],
      "source": [
        "vectors = TO_VEC(preproc_data)\n",
        "vectors.to_bin_counts()\n",
        "vectors.to_TFIDF()\n",
        "vectors.to_word_counts()\n",
        "vectors.to_word2vec()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "rK8JAqzQXNOb",
        "outputId": "b2f352e0-7f5a-4361-fcec-4a41c75c884f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_vec_just_tokenization</th>\n",
              "      <th>bin_vec_PorterStemmer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
              "      <th>bin_vec_good_preproc</th>\n",
              "      <th>TFIDF_just_tokenization</th>\n",
              "      <th>TFIDF_PorterStemmer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
              "      <th>...</th>\n",
              "      <th>word_counts_WordNetLemmatizer</th>\n",
              "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word_counts_PorterStemmer+misspellings</th>\n",
              "      <th>word_counts_good_preproc</th>\n",
              "      <th>word2vec_just_tokenization</th>\n",
              "      <th>word2vec_PorterStemmer</th>\n",
              "      <th>word2vec_WordNetLemmatizer</th>\n",
              "      <th>word2vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word2vec_PorterStemmer+misspellings</th>\n",
              "      <th>word2vec_good_preproc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.2853099, -0.09511143, 0.121866025, 0.244779...</td>\n",
              "      <td>[0.678889, -0.029011795, 0.37241903, 0.1751234...</td>\n",
              "      <td>[0.40553612, -0.009428509, 0.10449699, 0.23827...</td>\n",
              "      <td>[0.40553612, -0.009428509, 0.10449699, 0.23827...</td>\n",
              "      <td>[0.678889, -0.029011795, 0.37241903, 0.1751234...</td>\n",
              "      <td>[0.11180677, -0.25933897, 0.050498232, 0.11771...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.5349254, 0.22520572, 0.53480244, -0.2685251...</td>\n",
              "      <td>[0.14421621, 0.04486893, -0.043388907, -0.2901...</td>\n",
              "      <td>[0.5349254, 0.22520572, 0.53480244, -0.2685251...</td>\n",
              "      <td>[0.5349254, 0.22520572, 0.53480244, -0.2685251...</td>\n",
              "      <td>[0.14421621, 0.04486893, -0.043388907, -0.2901...</td>\n",
              "      <td>[-0.057604894, 0.020860864, -0.057774942, -0.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.41505033, 0.3681989, -0.41270638, 0.0710058...</td>\n",
              "      <td>[0.46806738, 0.30985337, -0.32477564, 0.217874...</td>\n",
              "      <td>[0.4595495, 0.42750767, -0.4832863, 0.08044413...</td>\n",
              "      <td>[0.4595495, 0.42750767, -0.4832863, 0.08044413...</td>\n",
              "      <td>[0.46806738, 0.30985337, -0.32477564, 0.217874...</td>\n",
              "      <td>[0.5824763, 0.45816827, -0.7259443, 0.01275363...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[-0.12468998, 0.46615756, 0.14391595, -0.59391...</td>\n",
              "      <td>[0.26392317, 0.21305189, 0.04008685, -0.847082...</td>\n",
              "      <td>[-0.29839906, 0.3007435, 0.09602404, -0.216149...</td>\n",
              "      <td>[-0.29839906, 0.3007435, 0.09602404, -0.216149...</td>\n",
              "      <td>[0.26392317, 0.21305189, 0.04008685, -0.847082...</td>\n",
              "      <td>[-0.21888603, 0.28482878, 0.3656059, -0.024685...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[-0.23556729, 0.10633588, 0.08997297, -0.64585...</td>\n",
              "      <td>[0.055927325, 0.15019982, 0.39600214, -0.81090...</td>\n",
              "      <td>[-0.23556729, 0.10633588, 0.08997297, -0.64585...</td>\n",
              "      <td>[-0.23556729, 0.10633588, 0.08997297, -0.64585...</td>\n",
              "      <td>[0.055927325, 0.15019982, 0.39600214, -0.81090...</td>\n",
              "      <td>[-0.23556729, 0.10633588, 0.08997297, -0.64585...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2740</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.45394692, 0.08919177, -0.24618256, -0.54535...</td>\n",
              "      <td>[0.52714056, 0.10494087, -0.4007467, -0.692048...</td>\n",
              "      <td>[0.5185543, 0.030394886, -0.41035488, -0.57915...</td>\n",
              "      <td>[0.5185543, 0.030394886, -0.41035488, -0.57915...</td>\n",
              "      <td>[0.52714056, 0.10494087, -0.4007467, -0.692048...</td>\n",
              "      <td>[0.5775512, 0.10704867, -0.52313066, -0.804033...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2741</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.20458242, -0.010020274, -0.20981666, -0.409...</td>\n",
              "      <td>[0.1850536, -0.25197196, -0.0046399706, -0.356...</td>\n",
              "      <td>[0.20458242, -0.010020274, -0.20981666, -0.409...</td>\n",
              "      <td>[0.20458242, -0.010020274, -0.20981666, -0.409...</td>\n",
              "      <td>[0.1850536, -0.25197196, -0.0046399706, -0.356...</td>\n",
              "      <td>[-0.10343051, -0.29656863, 0.00063971773, -0.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2742</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.6731785, 0.2555848, 0.1719926, -0.5569674, ...</td>\n",
              "      <td>[0.23924266, -0.12351985, -0.321273, -0.274652...</td>\n",
              "      <td>[0.24329011, 0.081628926, -0.14295039, -0.4845...</td>\n",
              "      <td>[0.24329011, 0.081628926, -0.14295039, -0.4845...</td>\n",
              "      <td>[0.23924266, -0.12351985, -0.321273, -0.274652...</td>\n",
              "      <td>[0.4834964, 0.091308095, -0.14050575, -0.46067...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2743</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.47406065, 0.344699, -0.52375674, -0.2275283...</td>\n",
              "      <td>[0.123244196, 0.22309582, -0.8785626, -0.16582...</td>\n",
              "      <td>[0.47406065, 0.344699, -0.52375674, -0.2275283...</td>\n",
              "      <td>[0.47406065, 0.344699, -0.52375674, -0.2275283...</td>\n",
              "      <td>[0.123244196, 0.22309582, -0.8785626, -0.16582...</td>\n",
              "      <td>[0.33609256, 0.2759547, -0.49915537, -0.219257...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2744</th>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.2207616, 0.4722776, -0.48748353, -0.7212909...</td>\n",
              "      <td>[0.044876087, 0.26666728, -0.40924096, -0.6126...</td>\n",
              "      <td>[0.2207616, 0.4722776, -0.48748353, -0.7212909...</td>\n",
              "      <td>[0.2207616, 0.4722776, -0.48748353, -0.7212909...</td>\n",
              "      <td>[0.044876087, 0.26666728, -0.40924096, -0.6126...</td>\n",
              "      <td>[0.0917867, 0.3830126, -0.46710637, -0.7230624...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2745 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              bin_vec_just_tokenization  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                  bin_vec_PorterStemmer  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                              bin_vec_WordNetLemmatizer  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                 bin_vec_WordNetLemmatizer+misspellings  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                     bin_vec_PorterStemmer+misspellings  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                   bin_vec_good_preproc  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                TFIDF_just_tokenization  \\\n",
              "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "                                    TFIDF_PorterStemmer  \\\n",
              "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "                                TFIDF_WordNetLemmatizer  \\\n",
              "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "                   TFIDF_WordNetLemmatizer+misspellings  ...  \\\n",
              "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "...                                                 ...  ...   \n",
              "2740  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "2741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "2742  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "2743  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "2744  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  ...   \n",
              "\n",
              "                          word_counts_WordNetLemmatizer  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "             word_counts_WordNetLemmatizer+misspellings  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                 word_counts_PorterStemmer+misspellings  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                               word_counts_good_preproc  \\\n",
              "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "...                                                 ...   \n",
              "2740  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2741  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2742  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2743  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2744  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                             word2vec_just_tokenization  \\\n",
              "0     [0.2853099, -0.09511143, 0.121866025, 0.244779...   \n",
              "1     [0.5349254, 0.22520572, 0.53480244, -0.2685251...   \n",
              "2     [0.41505033, 0.3681989, -0.41270638, 0.0710058...   \n",
              "3     [-0.12468998, 0.46615756, 0.14391595, -0.59391...   \n",
              "4     [-0.23556729, 0.10633588, 0.08997297, -0.64585...   \n",
              "...                                                 ...   \n",
              "2740  [0.45394692, 0.08919177, -0.24618256, -0.54535...   \n",
              "2741  [0.20458242, -0.010020274, -0.20981666, -0.409...   \n",
              "2742  [0.6731785, 0.2555848, 0.1719926, -0.5569674, ...   \n",
              "2743  [0.47406065, 0.344699, -0.52375674, -0.2275283...   \n",
              "2744  [0.2207616, 0.4722776, -0.48748353, -0.7212909...   \n",
              "\n",
              "                                 word2vec_PorterStemmer  \\\n",
              "0     [0.678889, -0.029011795, 0.37241903, 0.1751234...   \n",
              "1     [0.14421621, 0.04486893, -0.043388907, -0.2901...   \n",
              "2     [0.46806738, 0.30985337, -0.32477564, 0.217874...   \n",
              "3     [0.26392317, 0.21305189, 0.04008685, -0.847082...   \n",
              "4     [0.055927325, 0.15019982, 0.39600214, -0.81090...   \n",
              "...                                                 ...   \n",
              "2740  [0.52714056, 0.10494087, -0.4007467, -0.692048...   \n",
              "2741  [0.1850536, -0.25197196, -0.0046399706, -0.356...   \n",
              "2742  [0.23924266, -0.12351985, -0.321273, -0.274652...   \n",
              "2743  [0.123244196, 0.22309582, -0.8785626, -0.16582...   \n",
              "2744  [0.044876087, 0.26666728, -0.40924096, -0.6126...   \n",
              "\n",
              "                             word2vec_WordNetLemmatizer  \\\n",
              "0     [0.40553612, -0.009428509, 0.10449699, 0.23827...   \n",
              "1     [0.5349254, 0.22520572, 0.53480244, -0.2685251...   \n",
              "2     [0.4595495, 0.42750767, -0.4832863, 0.08044413...   \n",
              "3     [-0.29839906, 0.3007435, 0.09602404, -0.216149...   \n",
              "4     [-0.23556729, 0.10633588, 0.08997297, -0.64585...   \n",
              "...                                                 ...   \n",
              "2740  [0.5185543, 0.030394886, -0.41035488, -0.57915...   \n",
              "2741  [0.20458242, -0.010020274, -0.20981666, -0.409...   \n",
              "2742  [0.24329011, 0.081628926, -0.14295039, -0.4845...   \n",
              "2743  [0.47406065, 0.344699, -0.52375674, -0.2275283...   \n",
              "2744  [0.2207616, 0.4722776, -0.48748353, -0.7212909...   \n",
              "\n",
              "                word2vec_WordNetLemmatizer+misspellings  \\\n",
              "0     [0.40553612, -0.009428509, 0.10449699, 0.23827...   \n",
              "1     [0.5349254, 0.22520572, 0.53480244, -0.2685251...   \n",
              "2     [0.4595495, 0.42750767, -0.4832863, 0.08044413...   \n",
              "3     [-0.29839906, 0.3007435, 0.09602404, -0.216149...   \n",
              "4     [-0.23556729, 0.10633588, 0.08997297, -0.64585...   \n",
              "...                                                 ...   \n",
              "2740  [0.5185543, 0.030394886, -0.41035488, -0.57915...   \n",
              "2741  [0.20458242, -0.010020274, -0.20981666, -0.409...   \n",
              "2742  [0.24329011, 0.081628926, -0.14295039, -0.4845...   \n",
              "2743  [0.47406065, 0.344699, -0.52375674, -0.2275283...   \n",
              "2744  [0.2207616, 0.4722776, -0.48748353, -0.7212909...   \n",
              "\n",
              "                    word2vec_PorterStemmer+misspellings  \\\n",
              "0     [0.678889, -0.029011795, 0.37241903, 0.1751234...   \n",
              "1     [0.14421621, 0.04486893, -0.043388907, -0.2901...   \n",
              "2     [0.46806738, 0.30985337, -0.32477564, 0.217874...   \n",
              "3     [0.26392317, 0.21305189, 0.04008685, -0.847082...   \n",
              "4     [0.055927325, 0.15019982, 0.39600214, -0.81090...   \n",
              "...                                                 ...   \n",
              "2740  [0.52714056, 0.10494087, -0.4007467, -0.692048...   \n",
              "2741  [0.1850536, -0.25197196, -0.0046399706, -0.356...   \n",
              "2742  [0.23924266, -0.12351985, -0.321273, -0.274652...   \n",
              "2743  [0.123244196, 0.22309582, -0.8785626, -0.16582...   \n",
              "2744  [0.044876087, 0.26666728, -0.40924096, -0.6126...   \n",
              "\n",
              "                                  word2vec_good_preproc  \n",
              "0     [0.11180677, -0.25933897, 0.050498232, 0.11771...  \n",
              "1     [-0.057604894, 0.020860864, -0.057774942, -0.4...  \n",
              "2     [0.5824763, 0.45816827, -0.7259443, 0.01275363...  \n",
              "3     [-0.21888603, 0.28482878, 0.3656059, -0.024685...  \n",
              "4     [-0.23556729, 0.10633588, 0.08997297, -0.64585...  \n",
              "...                                                 ...  \n",
              "2740  [0.5775512, 0.10704867, -0.52313066, -0.804033...  \n",
              "2741  [-0.10343051, -0.29656863, 0.00063971773, -0.2...  \n",
              "2742  [0.4834964, 0.091308095, -0.14050575, -0.46067...  \n",
              "2743  [0.33609256, 0.2759547, -0.49915537, -0.219257...  \n",
              "2744  [0.0917867, 0.3830126, -0.46710637, -0.7230624...  \n",
              "\n",
              "[2745 rows x 24 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors.vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJHvynOXNOc"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jwXxyR-UXNOc"
      },
      "outputs": [],
      "source": [
        "def train(model, data, target):\n",
        "    accuracy_table = pd.DataFrame(columns=list(data.keys()))\n",
        "    models = {}\n",
        "    for i in range(data.shape[1]):\n",
        "        default_model = model\n",
        "        X = data.iloc[:,i]\n",
        "        name = X.name\n",
        "        y = target\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    train_size=0.8, \n",
        "                                                    random_state=42,\n",
        "                                                    shuffle = True)\n",
        "        X_train = pd.DataFrame(X_train.tolist())\n",
        "        X_test = pd.DataFrame(X_test.tolist())\n",
        "        y_train = y_train.tolist()\n",
        "        y_test = y_test.tolist()\n",
        "        \n",
        "        default_model.fit(X_train, y_train)\n",
        "        predict = default_model.predict(X_test)\n",
        "        accuracy_table.iloc[:,i] = [accuracy_score(predict, y_test)]\n",
        "        models[name] = default_model\n",
        "        \n",
        "    return accuracy_table, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NtEWqBnFXNOd"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "svc = svm.SVC()\n",
        "accuracy_table_svm, models_svm = train(svc, vectors.vectors, preproc_data.category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LmLDG6B0XNOe",
        "outputId": "4ac5d48e-1040-4978-b64b-80ccdb14f81f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_vec_just_tokenization</th>\n",
              "      <th>bin_vec_PorterStemmer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
              "      <th>bin_vec_good_preproc</th>\n",
              "      <th>TFIDF_just_tokenization</th>\n",
              "      <th>TFIDF_PorterStemmer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
              "      <th>...</th>\n",
              "      <th>word_counts_WordNetLemmatizer</th>\n",
              "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word_counts_PorterStemmer+misspellings</th>\n",
              "      <th>word_counts_good_preproc</th>\n",
              "      <th>word2vec_just_tokenization</th>\n",
              "      <th>word2vec_PorterStemmer</th>\n",
              "      <th>word2vec_WordNetLemmatizer</th>\n",
              "      <th>word2vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word2vec_PorterStemmer+misspellings</th>\n",
              "      <th>word2vec_good_preproc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.907104</td>\n",
              "      <td>0.907104</td>\n",
              "      <td>0.907104</td>\n",
              "      <td>0.907104</td>\n",
              "      <td>0.907104</td>\n",
              "      <td>0.897996</td>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.907104</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903461</td>\n",
              "      <td>0.903461</td>\n",
              "      <td>0.905282</td>\n",
              "      <td>0.897996</td>\n",
              "      <td>0.82878</td>\n",
              "      <td>0.783242</td>\n",
              "      <td>0.82878</td>\n",
              "      <td>0.82878</td>\n",
              "      <td>0.783242</td>\n",
              "      <td>0.781421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   bin_vec_just_tokenization  bin_vec_PorterStemmer  \\\n",
              "0                   0.907104               0.907104   \n",
              "\n",
              "   bin_vec_WordNetLemmatizer  bin_vec_WordNetLemmatizer+misspellings  \\\n",
              "0                   0.907104                                0.907104   \n",
              "\n",
              "   bin_vec_PorterStemmer+misspellings  bin_vec_good_preproc  \\\n",
              "0                            0.907104              0.897996   \n",
              "\n",
              "   TFIDF_just_tokenization  TFIDF_PorterStemmer  TFIDF_WordNetLemmatizer  \\\n",
              "0                 0.899818             0.907104                 0.901639   \n",
              "\n",
              "   TFIDF_WordNetLemmatizer+misspellings  ...  word_counts_WordNetLemmatizer  \\\n",
              "0                              0.901639  ...                       0.903461   \n",
              "\n",
              "   word_counts_WordNetLemmatizer+misspellings  \\\n",
              "0                                    0.903461   \n",
              "\n",
              "   word_counts_PorterStemmer+misspellings  word_counts_good_preproc  \\\n",
              "0                                0.905282                  0.897996   \n",
              "\n",
              "   word2vec_just_tokenization  word2vec_PorterStemmer  \\\n",
              "0                     0.82878                0.783242   \n",
              "\n",
              "   word2vec_WordNetLemmatizer  word2vec_WordNetLemmatizer+misspellings  \\\n",
              "0                     0.82878                                  0.82878   \n",
              "\n",
              "   word2vec_PorterStemmer+misspellings  word2vec_good_preproc  \n",
              "0                             0.783242               0.781421  \n",
              "\n",
              "[1 rows x 24 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_table_svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RY4PdV9kXNOf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/banji/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/banji/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/banji/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "accuracy_table_logreg, models_logreg = train(lr, vectors.vectors, preproc_data.category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uPQbHkxnXNOg",
        "outputId": "d00ef853-a5b6-40cb-b4c9-5c7d896aecc4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_vec_just_tokenization</th>\n",
              "      <th>bin_vec_PorterStemmer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
              "      <th>bin_vec_good_preproc</th>\n",
              "      <th>TFIDF_just_tokenization</th>\n",
              "      <th>TFIDF_PorterStemmer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
              "      <th>...</th>\n",
              "      <th>word_counts_WordNetLemmatizer</th>\n",
              "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word_counts_PorterStemmer+misspellings</th>\n",
              "      <th>word_counts_good_preproc</th>\n",
              "      <th>word2vec_just_tokenization</th>\n",
              "      <th>word2vec_PorterStemmer</th>\n",
              "      <th>word2vec_WordNetLemmatizer</th>\n",
              "      <th>word2vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word2vec_PorterStemmer+misspellings</th>\n",
              "      <th>word2vec_good_preproc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.903461</td>\n",
              "      <td>0.908925</td>\n",
              "      <td>0.905282</td>\n",
              "      <td>0.905282</td>\n",
              "      <td>0.908925</td>\n",
              "      <td>0.905282</td>\n",
              "      <td>0.910747</td>\n",
              "      <td>0.918033</td>\n",
              "      <td>0.908925</td>\n",
              "      <td>0.908925</td>\n",
              "      <td>...</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.82878</td>\n",
              "      <td>0.765027</td>\n",
              "      <td>0.830601</td>\n",
              "      <td>0.830601</td>\n",
              "      <td>0.765027</td>\n",
              "      <td>0.763206</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   bin_vec_just_tokenization  bin_vec_PorterStemmer  \\\n",
              "0                   0.903461               0.908925   \n",
              "\n",
              "   bin_vec_WordNetLemmatizer  bin_vec_WordNetLemmatizer+misspellings  \\\n",
              "0                   0.905282                                0.905282   \n",
              "\n",
              "   bin_vec_PorterStemmer+misspellings  bin_vec_good_preproc  \\\n",
              "0                            0.908925              0.905282   \n",
              "\n",
              "   TFIDF_just_tokenization  TFIDF_PorterStemmer  TFIDF_WordNetLemmatizer  \\\n",
              "0                 0.910747             0.918033                 0.908925   \n",
              "\n",
              "   TFIDF_WordNetLemmatizer+misspellings  ...  word_counts_WordNetLemmatizer  \\\n",
              "0                              0.908925  ...                       0.901639   \n",
              "\n",
              "   word_counts_WordNetLemmatizer+misspellings  \\\n",
              "0                                    0.901639   \n",
              "\n",
              "   word_counts_PorterStemmer+misspellings  word_counts_good_preproc  \\\n",
              "0                                0.901639                  0.901639   \n",
              "\n",
              "   word2vec_just_tokenization  word2vec_PorterStemmer  \\\n",
              "0                     0.82878                0.765027   \n",
              "\n",
              "   word2vec_WordNetLemmatizer  word2vec_WordNetLemmatizer+misspellings  \\\n",
              "0                    0.830601                                 0.830601   \n",
              "\n",
              "   word2vec_PorterStemmer+misspellings  word2vec_good_preproc  \n",
              "0                             0.765027               0.763206  \n",
              "\n",
              "[1 rows x 24 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_table_logreg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZUIlydJwXNOg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier()\n",
        "# parameters = {'max_depth':[3,10]} #лучший 10\n",
        "# gsCV = GridSearchCV(gbc, parameters)\n",
        "accuracy_table_gbc, models_gbc = train(gbc, vectors.vectors, preproc_data.category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "koPjWLy_XNOg",
        "outputId": "36f24b17-8d27-4611-ca4d-78827a91a3ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin_vec_just_tokenization</th>\n",
              "      <th>bin_vec_PorterStemmer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer</th>\n",
              "      <th>bin_vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>bin_vec_PorterStemmer+misspellings</th>\n",
              "      <th>bin_vec_good_preproc</th>\n",
              "      <th>TFIDF_just_tokenization</th>\n",
              "      <th>TFIDF_PorterStemmer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer</th>\n",
              "      <th>TFIDF_WordNetLemmatizer+misspellings</th>\n",
              "      <th>...</th>\n",
              "      <th>word_counts_WordNetLemmatizer</th>\n",
              "      <th>word_counts_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word_counts_PorterStemmer+misspellings</th>\n",
              "      <th>word_counts_good_preproc</th>\n",
              "      <th>word2vec_just_tokenization</th>\n",
              "      <th>word2vec_PorterStemmer</th>\n",
              "      <th>word2vec_WordNetLemmatizer</th>\n",
              "      <th>word2vec_WordNetLemmatizer+misspellings</th>\n",
              "      <th>word2vec_PorterStemmer+misspellings</th>\n",
              "      <th>word2vec_good_preproc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.905282</td>\n",
              "      <td>0.892532</td>\n",
              "      <td>0.897996</td>\n",
              "      <td>0.894353</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>0.901639</td>\n",
              "      <td>...</td>\n",
              "      <td>0.903461</td>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.899818</td>\n",
              "      <td>0.89071</td>\n",
              "      <td>0.825137</td>\n",
              "      <td>0.770492</td>\n",
              "      <td>0.830601</td>\n",
              "      <td>0.82878</td>\n",
              "      <td>0.770492</td>\n",
              "      <td>0.766849</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   bin_vec_just_tokenization  bin_vec_PorterStemmer  \\\n",
              "0                   0.899818               0.899818   \n",
              "\n",
              "   bin_vec_WordNetLemmatizer  bin_vec_WordNetLemmatizer+misspellings  \\\n",
              "0                   0.901639                                0.901639   \n",
              "\n",
              "   bin_vec_PorterStemmer+misspellings  bin_vec_good_preproc  \\\n",
              "0                            0.905282              0.892532   \n",
              "\n",
              "   TFIDF_just_tokenization  TFIDF_PorterStemmer  TFIDF_WordNetLemmatizer  \\\n",
              "0                 0.897996             0.894353                 0.901639   \n",
              "\n",
              "   TFIDF_WordNetLemmatizer+misspellings  ...  word_counts_WordNetLemmatizer  \\\n",
              "0                              0.901639  ...                       0.903461   \n",
              "\n",
              "   word_counts_WordNetLemmatizer+misspellings  \\\n",
              "0                                    0.899818   \n",
              "\n",
              "   word_counts_PorterStemmer+misspellings  word_counts_good_preproc  \\\n",
              "0                                0.899818                   0.89071   \n",
              "\n",
              "   word2vec_just_tokenization  word2vec_PorterStemmer  \\\n",
              "0                    0.825137                0.770492   \n",
              "\n",
              "   word2vec_WordNetLemmatizer  word2vec_WordNetLemmatizer+misspellings  \\\n",
              "0                    0.830601                                  0.82878   \n",
              "\n",
              "   word2vec_PorterStemmer+misspellings  word2vec_good_preproc  \n",
              "0                             0.770492               0.766849  \n",
              "\n",
              "[1 rows x 24 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_table_gbc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-5PvALjXNOh"
      },
      "source": [
        "# Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMNWpGlWXNOh"
      },
      "source": [
        "top-10 most similar pair of tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E_JyWu2XNOh",
        "outputId": "0351d7c2-04ba-411a-e144-9d1c0a3240fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[159, 214, 1.0000000000000002],\n",
              " [179, 851, 1.0000000000000002],\n",
              " [214, 159, 1.0000000000000002],\n",
              " [437, 711, 1.0000000000000002],\n",
              " [574, 159, 1.0000000000000002],\n",
              " [608, 679, 1.0000000000000002],\n",
              " [654, 690, 1.0000000000000002],\n",
              " [679, 608, 1.0000000000000002],\n",
              " [690, 654, 1.0000000000000002],\n",
              " [711, 437, 1.0000000000000002]]"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "matrix_similarity = []\n",
        "data_1 = vectors.vectors.TFIDF_good_preproc\n",
        "data_2 = vectors.vectors.TFIDF_WordNetLemmatizer\n",
        "\n",
        "matrix_similarity = cosine_similarity(list(data_1))\n",
        "cos_sim = []\n",
        "for i in range(len(matrix_similarity)):\n",
        "    for j in range(len(matrix_similarity[i])):\n",
        "        if (i == j): matrix_similarity[i][j] = -np.inf\n",
        "    a = list([i, np.argmax(matrix_similarity[i]),max(matrix_similarity[i])])\n",
        "    cos_sim.append(a)\n",
        "\n",
        "cos_sim.sort(key = lambda x: -x[2])\n",
        "\n",
        "cos_sim[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TetLs0_IXNOh",
        "outputId": "118f344c-d58e-460a-f147-af3ec007b9b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('I miss you unhappy ', 'i miss them unhappy ')"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors.text.tweets[159], vectors.text.tweets[214]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmPh0cvKXNOi",
        "outputId": "249d03cd-b077-4720-9315-737e3529e42e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('babies unhappy  ', 'baby unhappy  ')"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors.text.tweets[711], vectors.text.tweets[437]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voSBp92EXNOi"
      },
      "outputs": [],
      "source": [
        "def predict_class(text, model, vectorizer):\n",
        "    vectors = vectorizer.transform(text)\n",
        "    return model.predict(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOW9f6sZXNOj",
        "outputId": "55a5c389-3453-429d-bbe1-a421de39db26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_class([\"i love you\", \"fuck\", 'abra abrab ab', 'bad bad bad bad bad', 'thank you very much you helped me'], gbc, vectors.vectorizers['TFIDF_good_preproc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HiE3FCuXNOj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFkTK0UGXNOj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def plot_conf_matrix(true, pred, cat=['-1', '0', '1']):\n",
        "#     sns.heatmap(data=confusion_matrix(true, pred), \n",
        "#             annot=True, fmt=\"d\", cbar=False, xticklabels=cat, yticklabels=cat)\n",
        "\n",
        "\n",
        "# plot_conf_matrix(y_test, tfidf_pred_data)\n",
        "\n",
        "# accuracy_score(y_test, tfidf_pred_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLiLPGWRXNOj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHXcbxlsXNOj"
      },
      "outputs": [],
      "source": [
        "#понадобится для nltk.download()\n",
        "# import nltk\n",
        "# import ssl\n",
        "\n",
        "# try:\n",
        "#     _create_unverified_https_context = ssl._create_unverified_context\n",
        "# except AttributeError:\n",
        "#     pass\n",
        "# else:\n",
        "#     ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Z4-Pe0XNOl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tweets.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
